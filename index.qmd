---
title: "Bayesian VAR Forecast of Economic Activity using Macroeconomic and Financial Variables"
author: "Carl Buan"

execute:
  echo: true

# format:
#   pdf:
#     fontfamily: cmbright
#     geometry: margin=1.5cm
format:
  html:
    toc: true
    toc-location: left
---

> **Abstract**

# Research question
Past financial crises such as in 2008 have shown the severe impact that financial conditions can have on the real economy with crashes followed by increasing unemployment and decreased economic activity. This research project's aim to analyze the forecasting performance of macroeconomic variables by including financial variables to forecast GDP.
 
Thus the research question can be stated as: Will including financial variables improve the forecasting performance of economic activity?

## Motivation

Traditional macroeconomic modelling often exclude financial markets, as the effects from the financial nominal variables on real variables are deemed insignificant per the classical dichotomy in macroeconomics. However, as earlier studies have shown and the descriptive analysis following this will illustrate, changes in the financial sector can spill over to economic activity. Therefore, it could help explaining the real economy, which could translate to improved forecasting performance.

As both monetary and fiscal policy is not only based on current economic conditions but also expectations regarding the future, it is essential to be able to generate accurate predictions of economic activity and inflation in the future. 

# Data and its properties
My choice of variables is somewhat inspired by papers like Peersman et. al (2017), who perform SVAR analyses of the ECB's monetary policy, however, I am instead interested in examining the forecasting ability of macroeconomic variables combined with financial for the euro area. 
As forecasts are often short-term forecasts of the immediate future, data frequency has to be high, hence all data used in this analysis is monthly.

The economic variables used in this analysis are then:

To model the economic activity in the euro area, I use the Industrial Production as proxy for the movements in the Gross Domestic Product (GDP). If I instead opted to use GDP as a variable directly, I would have been required to apply temporal disaggregation, as GDP is tracked quarterly.

The price level in the euro area is included as the Harmonized Index of Consumer Prices (HICP), which measures the price level and inflation across the euro area. Including the price level is important, as it is a major concern for policy makers and it will affect the economic activity and be affected by financial conditions. The base year of the HICP is 2015.

Additionally, I include unemployment in the euro area, as it is both an indicator of economic activity and current expectations to the future. Hence, unemployment serves as an important variable to include when modelling the economy.

I chose to include financial stress, which is measured by composite indicator variable for systemic stress (CISS) in the euro area, as financial conditions and markets highly affects the economic activity, inflation and unemployment.

I include the M2 money supply in the euro area to include the most liquid part of the overall money supply and short term deposits, as it illustrates the available economic resources in the economy.

The key ECB interest rate the Main Refinancing Operations (MRO) is included as well, which state the interest that banks have to pay, if they want to borrow money for a week. 

Finally, I include private consumption to measure how much money households are spending, which illustrate the economic activity.


The used time series are collected from the ECB's data warehouse with the chosen time period 01.01.2003 to 2023.12.01. I use the package **ecb** to collect the data.

## Descriptive analysis
The five variables are visualized in @fig-tsplots, where I have taken the logarithm to all variables except from the indicator variable CISS. Inflation and M2 money supply seem to follow an upwards trend, while industrial production, financial stress and unemployment all seem to follow each other, such that industrial production is negative correlated with unemployment and financial stress. Intuitively it makes sense that economic activity decreases with increasing unemployment or stress and uncertainty in the financial sector.

```{r package load}
#| echo: false
#| message: false
#| warning: false
#| results: hide

  
library(dplyr)
library(tidyr)
library(ggplot2)
library(ecb)
library(gridExtra)
library(bsvars)
library(lubridate)
library(tseries)
library(zoo)
library(tempdisagg)

```

```{r Downloading and readying data}
#| echo: false
#| message: false
#| warning: false
#| results: hide


rm(list = ls())

# Downloading Data
start_date_m <- "2003-01" 
end_date_m <- "2023-12"

# Define the datasets
series_info <- list(
  hicp = "ICP.M.U2.Y.000000.3.INX",
  ciss = "CISS.M.U2.Z0Z.4F.EC.SOV_EW.IDX",
  ip = "STS.M.I8.Y.PROD.NS0010.4.000",
  un = "LFSI.M.I9.S.UNEHRT.TOTAL0.15_74.T",
  m2 = "BSI.M.U2.Y.V.M20.X.1.U2.2300.Z01.E"
)

# Making function to collect data
prepare_data <- function(series_id, name, start_date, end_date) {
  get_data(series_id, filter = list(startPeriod = start_date, endPeriod = end_date)) |>
    transmute(obstime, !!name := obsvalue)
}

# Collecting data
datasets <- lapply(names(series_info), function(name) {
  prepare_data(series_info[[name]], name, start_date_m, end_date_m)
})

# Collecting daily data
mro <- get_data("FM.B.U2.EUR.4F.KR.MRR_FR.LEV") |>
  mutate(obstime = as.Date(obstime))

# Creating a full sequence of dates for the range and merging with original data
mro <- data.frame(obstime = seq(from = as.Date("2003-01-01"), to = as.Date("2023-12-31"), by = "day")) |>
  left_join(mro, by = "obstime") |>
  mutate(obsvalue = na.locf(obsvalue, na.rm = FALSE)) |>
  mutate(obsvalue = na.locf(obsvalue, fromLast = TRUE, na.rm = FALSE)) |>
  select(obstime, obsvalue)

# Transforming to monthly data
mro <- mro |>
  group_by(obstime = as.yearmon(obstime)) |>
  summarize(mro = last(obsvalue)) |>
  mutate(obstime = as.Date(obstime))

# Collecting quarterly data - BAD CODE - IGNORE IT
consumption <- get_data("MNA.Q.Y.I9.W0.S1M.S1.D.P31._Z._Z._T.EUR.V.N") |>
  select(obstime, obsvalue) |>
  slice(-1:-32)  # Removes the first years
consumption_ts <- ts(consumption$obsvalue, start = c(2003, 1), frequency = 4)
monthly_consumption <- td(consumption_ts ~1, to = "monthly", method = "chow-lin-maxlog", conversion = "average")
start_date <- as.Date("2003-01-01")
monthly_dates <- seq(from = start_date, by = "month", length = length(monthly_consumption$values))
consumption <- data.frame(obstime = monthly_dates, consumption = log(monthly_consumption$values) )


# Merge all datasets by 'obstime'
Data_function <- Reduce(function(x, y) left_join(x, y, by = "obstime"), datasets)

Data <- Data_function |>
  mutate(across(c(2, 4, 6), ~log(.)))

# Convert 'obstime' to Date format
Data$obstime <- as.Date(paste0(Data$obstime, "-01"))

# Appending data
Data <- Data |>
  left_join(mro, by = "obstime") |>
  left_join(consumption, by = "obstime") |>
  select(obstime, ip, everything())

```

```{r Plotting data}
#| echo: false
#| message: false
#| warning: false
#| results: hide


# Define the plot configurations
plot_settings <- list(
  list(name = "ip", title = "Industrial production", ylab = "LOG(IP)"),
  list(name = "hicp", title = "Inflation", ylab = "LOG(HICP)"),
  list(name = "ciss", title = "Financial Stress", ylab = "CISS"),
  list(name = "un", title = "Unemployment", ylab = "UN"),
  list(name = "m2", title = "M2 - Money Stock", ylab = "LOG(M2)"),
  list(name = "mro", title = "ECB interest rate", ylab ="MRO"),
  list(name = "consumption", title = "Private Consumption Euro Area", ylab="LOG(C)")
)

# Create the plots
plots <- lapply(plot_settings, function(setting) {
  ggplot(Data, aes_string(x = "obstime", y = setting$name)) +
    geom_line() +
    labs(x = "", y = setting$ylab, title = setting$title) +
    theme_bw() +
    theme(
      panel.border = element_rect(colour = "black", fill=NA),  
      panel.grid.major = element_blank(),  
      panel.grid.minor = element_blank(),  
      plot.background = element_blank(),  
      plot.title = element_text(hjust = 0.5)  
    )
})


```

```{r Plotting the variables}
#| echo: false
#| message: false
#| warning: false
#| label: fig-tsplots
#| fig-cap: Plots of time series

grid.arrange(grobs = plots, ncol = 2)
```

## Autocorrelation in the variables

Additionally, I check for autocorrelation in the time series in @fig-acf and @fig-pacf, where there is clearly a high degree of memory in the variables.

```{r ACF and PACF}
#| echo: false
#| message: false
#| warning: false
#| label: fig-acf
#| fig-cap: Autocorrelation plots for the variables

# Change to time series
Data <- as.ts(Data)


# ACF and PACF
par(mfrow = c(3, 3))

variable_names <- c("ip", "hicp", "ciss", "un", "m2", "mro", "consumption")

# Loop through the columns and create ACF plots
for (i in seq_along(variable_names)) {
  # Compute and plot ACF
  acf(Data[, i], main = paste("ACF for", variable_names[i]))
}
  
```

```{r PACF}
#| echo: false
#| message: false
#| warning: false
#| label: fig-pacf
#| fig-cap: Partical autocorrelation plots for the variables

par(mfrow = c(3, 3))
for (i in seq_along(variable_names)) {
  # Compute and plot PACF
  pacf(Data[, i], main = paste("PACF for", variable_names[i]))
}

```


## Unit root tests ##
To test for the timeseries being stationary, I apply an Augmented Dickey Fuller (ADF) test, by using the **adf()**, which tests for the presence an unit root in the time series with the chosen lag of 12 periods, since the data is monthly. @tbl-adf reports the test statistics for the ADF tests:
```{r ADF tests}
#| echo: false
#| message: false
#| warning: false
#| results: hide


# ADF Test

for (i in seq_along(variable_names)) {
  variable_name <- paste("adf", variable_names[i], sep = "_")
  assign(variable_name, adf.test(Data[, i], k=12, alternative = "stationary"))
}

adf_results <- data.frame(
  Variable = c("IP", "HICP", "CISS", "UN", "M2", "MRO", "Consumption"),
  ADF_Statistic = c(adf_hicp$statistic, adf_ip$statistic, adf_ciss$statistic, adf_un$statistic, adf_m2$statistic, adf_mro$statistic, adf_consumption$statistic),
  P_Value = c(adf_hicp$p.value, adf_ip$p.value, adf_ciss$p.value, adf_un$p.value, adf_m2$p.value, adf_mro$p.value, adf_consumption$p.value),
  lags = rep(12, 7)
)

```

```{r}
#| label: tbl-adf
#| tbl-cap: ADF tests' statistics with the null hypothesis being a unit root
#| echo: false

knitr::kable(adf_results, digits = 3, align = 'c')
```
Only for inflation can I reject the possibility of an unit root being present at the 5 pct. confidence interval.

# The model
To analyze the macroeconomic and financial variables and run forecasts, a vector autoregression (VAR) model can be applied, which allows for the multivariate framework of several timeseries. 
A general VAR model with $\rho$ lags for $t=1,...,T$ can be stated as:

\begin{gather}
y_{t}	=\mu_{0}+A_{1}y_{t-1}+...+A_{\rho}y_{t-\rho}+\epsilon_{t}
\epsilon_{t}|Y_{T-1}	\sim iid\left(0_{N},\Sigma\right)
\end{gather}

Where $y_{t}=N\times1$ is a vector of observations at time $t$, $\mu_{0}=N\times1$ is a vector of constant terms, $A_{i}=N\times N$ is a vector of the autoregressive slope parameters, $\epsilon_{t}=N\times1$ is a vector of error terms, $Y_{t-1}$ is the information set and $\Sigma=N\times N$ is the covariance matrix of the error term.    

In matrix form, I have the model as with N=7 variables:
\begin{gather}
Y	=XA+E
E|X	\sim\mathcal{MN}_{T\times N}\left(0_{T\times N},\Sigma,I_{T}\right)
\end{gather}

Where $Y=T\times7$ matrix, $X=T\times\left(1+\left(7\times p\right)\right)$, $A=\left(1+\left(7\times p\right)\right)\times6$ matrix that has the relationships between the used variables and $E=T\times7$ matrix of error terms. $p=12$ which is the amount of lags for monthly data and in total I have 252 observations of monthly data.

## Matrix version of model
In matrix form, I have the model as with $N=7$ variables:

\begin{gather}
Y	=XA+E
\end{gather}

\begin{gather}
E|X	\sim\mathcal{MN}_{T\times N}\left(0_{T\times N},\Sigma,I_{T}\right)
\end{gather}

Where $Y=T\times7$ matrix, $X=T\times\left(1+\left(7\times p\right)\right)$ matrix, $A=\left(1+\left(7\times p\right)\right)\times7$ matrix that has the relationships between the used variables and $E=T\times7$ matrix of error terms. p=12 which is the amount of lags for monthly data and in total I have 252 observations of monthly data.

# Baseline model:
Then the likelihood function is given by:

\begin{align}
Y|X,A,\Sigma&\sim\mathcal{MN}_{T\times N}\left(XA,\Sigma,I_{T}\right)\\L\left(A,\Sigma|Y,X\right)&\propto\det\left(\Sigma\right)^{-\frac{T}{2}}\exp\left(-\frac{1}{2}tr\left[\Sigma^{-1}\left(Y-XA\right)'\left(Y-XA\right)\right]\right)
\end{align}


The natural conjugate priors for A and \Sigma are then assumed to follow matrix-variate normal and inverse Wishart distributions:

\begin{align}
p\left(A,\Sigma\right)&=p\left(A|\Sigma\right)p\left(\Sigma\right)\\A|\Sigma&\sim\mathcal{MN}_{T\times N}\left(\underline{A},\Sigma,\underline{V}\right)\\\Sigma&\sim IW_{N}\left(\underline{S},\underline{\nu}\right)
  \end{align}


Which follow the Minnesota prior set as:

\begin{align}
\underline{A}&=\left[0_{N\times1},I_{N},0_{N\times\left(p-1\right)N}\right]'\\\underline{V}&=diag\left(\left[\kappa_{2},\kappa_{1}\left(p^{-2}\otimes1_{N}^{'}\right)\right]\right)\\\underline{\nu}&=N+1
\end{align}

Then the posterior distribution will be:

\begin{align}
p\left(A,\Sigma|Y,X\right)&\propto\det\left(\Sigma\right)^{-\frac{T}{2}}\\&\times\exp\left(-\frac{1}{2}tr\left[\Sigma^{-1}\left(A-\hat{A}\right)'X'X\left(A-\hat{A}\right)\right]\right)\\&\times\exp\left(-\frac{1}{2}tr\left[\Sigma^{-1}\left(Y-X\hat{A}\right)'\left(Y-X\hat{A}\right)\right]\right)\\&\times\det\left(\Sigma\right)^{-\frac{N+K+\underline{\nu}+1}{2}}\\&\times\exp\left(-\frac{1}{2}tr\left[\Sigma^{-1}\left(A-\underline{A}\right)'\underline{V}^{-1}\left(A-\underline{A}\right)\right]\right)\\&\times\exp\left(-\frac{1}{2}tr\left[\Sigma^{-1}\underline{S}\right]\right)
\end{align}

Then by combining the terms yields the posterior distributions for $A$ and $\Sigma$ as:

\begin{align}
p\left(A,\Sigma|Y,X\right)&\propto\det\left(\Sigma\right)^{-\frac{T+N+K+\underline{\nu}+1}{2}}\\&\times\exp\left(-\frac{1}{2}tr\left[\Sigma^{-1}\left[\left(A-\hat{A}\right)'X'X\left(A-\hat{A}\right)+\left(Y-X\hat{A}\right)'\left(Y-X\hat{A}\right)\right.\right.\right.\\&+\left.\left.\left.\left(A-\underline{A}\right)'\underline{V}^{-1}\left(A-\underline{A}\right)+\underline{S}\right)\right]\right]
\end{align}

Writing all the squares out yields the full conditional as:

\begin{align}
p\left(A,\Sigma|Y,X\right)&=p\left(A|Y,X,\Sigma\right)p\left(\Sigma|Y,X\right)\\p\left(A|Y,X,\Sigma\right)&=\mathcal{MN}_{T\times N}\left(\overline{A},\Sigma,\overline{V}\right)\\p\left(\Sigma|Y,X\right)&=IW_{N}\left(\overline{S},\overline{\nu}\right)\\\overline{V}&=\left(X'X+V^{-1}\right)^{-1}\\\overline{A}&=\overline{V}\left(X'Y+\underline{V}^{-1}\underline{A}\right)\\\overline{\nu}&=T+\underline{\nu}\\\overline{S}&=\underline{S}+Y'Y+\underline{A}'\underline{V}^{-1}\underline{A}-\overline{A}'\overline{V}^{-1}\overline{A}
\end{align}

The baseline model can be estimated by:

```{r}
#| echo: true
#| message: false
#| warning: false
#| results: hide

### Baseline model BVAR estimation:

# Setting specifications
N = ncol(Data[ , -1])
p = 12
K = 1+N*p
S = c(5000,25000)
set.seed(1)

# Initializing X and Y matrices
y       = ts(Data[ , -1], start=c(2003,1), frequency=12)
Y       = ts(y[13:nrow(y),], start=c(2004,1), frequency=12)
X       = matrix(1,nrow(Y),1)
for (i in 1:p){
  X     = cbind(X,y[13:nrow(y)-i,])
}


# Maximum Likelihood Estimator
A.hat       = solve(t(X)%*%X)%*%t(X)%*%Y
Sigma.hat   = t(Y-X%*%A.hat)%*%(Y-X%*%A.hat)/T

# Setting Minnesota Prior
kappa.1           = 0.02^2
kappa.2           = 100
A.prior           = matrix(0,nrow(A.hat),ncol(A.hat))
A.prior[2:(N+1),] = diag(N)

priors = list(
  A.prior     = A.prior,
  V.prior     = diag(c(kappa.2,kappa.1*((1:p)^(-2))%x%rep(1,N))),
  S.prior     = diag(diag(Sigma.hat)),
  nu.prior    = N+1 
)

# BVAR function

BVAR = function(Y,X,priors,S){
  
  # normal-inverse Wishart posterior parameters
  V.bar.inv   = t(X)%*%X + diag(1/diag(priors$V.prior))
  V.bar       = solve(V.bar.inv)
  A.bar       = V.bar%*%(t(X)%*%Y + diag(1/diag(priors$V.prior))%*%priors$A.prior)
  nu.bar      = nrow(Y) + priors$nu.prior
  S.bar       = priors$S.prior + t(Y)%*%Y + t(priors$A.prior)%*%diag(1/diag(priors$V.prior))%*%priors$A.prior - t(A.bar)%*%V.bar.inv%*%A.bar
  S.bar.inv   = solve(S.bar)
  
  #posterior draws
  Sigma.posterior   = rWishart(sum(S), df=nu.bar, Sigma=S.bar.inv)
  Sigma.posterior   = apply(Sigma.posterior,3,solve)
  Sigma.posterior   = array(Sigma.posterior,c(N,N,sum(S)))
  A.posterior       = array(rnorm(prod(c(dim(A.bar),sum(S)))),c(dim(A.bar),sum(S)))
  L                 = t(chol(V.bar))
  
  for (s in 1:sum(S)){
    A.posterior[,,s]= A.bar + L%*%A.posterior[,,s]%*%chol(Sigma.posterior[,,s])
  }
  
  posterior = list(
    Sigma.posterior   = Sigma.posterior,
    A.posterior       = A.posterior
  )
  return(posterior)
}

# Applying BVAR function
posterior.draws = BVAR(Y=Y, X=X, priors=priors, S=S)
round(apply(posterior.draws$Sigma.posterior, 1:2, mean),3)
round(apply(posterior.draws$A.posterior, 1:2, mean),3)

```


The posterior mean of the matrices are close to an identity matrix and for the constant term, it is close to a zero vector. Hence, the estimation appears robust.

# Extended model

Currently empty until new extension found.

```{r}
#| echo: true
#| message: false
#| warning: false
#| results: hide

### Extended model BVAR estimation - Not inserted as extension will be switched


```


# Bayesian VAR Stochastic Volatility

The baseline model is changed to allow for stochastic volatility, such that we specify the conditional heteroskedasticity:

\begin{align}
Y&=XA+E\\E|X&\sim\mathcal{MN}_{T\times N}\left(0_{T\times N},\Sigma,\text{diag}\left(\sigma^{2}\right)\right)
\end{align}

This gives the new likelihood function:

\begin{align}
Y|X,A,\Sigma&\sim\mathcal{MN}_{T\times N}\left(XA,\Sigma,\text{diag}\left(\sigma^{2}\right)\right)\\L\left(A,\Sigma|Y,X\right)&\propto\det\left(\Sigma\right)^{-\frac{T}{2}}\exp\left(-\frac{1}{2}\text{tr}\left(\Sigma^{-1}\left(Y-XA\right)'\text{diag}\left(\sigma^{2}\right)\left(Y-XA\right)\right)\right)
\end{align}

Which can be estimated by:

```{r}
#| echo: true
#| message: false
#| warning: false
#| results: hide

### Extended model BVAR estimation - Not inserted as extension will be switched

### Stochastic volatility

library(mgcv)

SVcommon.Gibbs.iteration = function(aux, priors){
  # A single iteration of the Gibbs sampler for the SV component
  #
  # aux is a list containing:
  #   Y - a TxN matrix
  #   X - a TxK matrix
  #   H - a Tx1 matrix
  #   h0 - a scalar
  #   sigma.v2 - a scalar
  #   s - a Tx1 matrix
  #   A - a KxN matrix
  #   Sigma - an NxN matrix
  #   sigma2 - a Tx1 matrix
  #
  # priors is a list containing:
  #   h0.v - a positive scalar
  #   h0.m - a scalar
  #   sigmav.s - a positive scalar
  #   sigmav.nu - a positive scalar
  #   HH - a TxT matrix
  
  T             = dim(aux$Y)[1]
  N             = dim(aux$Y)[2]
  alpha.st      = c(1.92677,1.34744,0.73504,0.02266,0-0.85173,-1.97278,-3.46788,-5.55246,-8.68384,-14.65000)
  sigma.st      = c(0.11265,0.17788,0.26768,0.40611,0.62699,0.98583,1.57469,2.54498,4.16591,7.33342)
  pi.st         = c(0.00609,0.04775,0.13057,0.20674,0.22715,0.18842,0.12047,0.05591,0.01575,0.00115)
  
  Lambda        = solve(chol(aux$Sigma))
  Z             = rowSums( ( aux$Y - aux$X %*% aux$A ) %*% Lambda ) / sqrt(N)
  Y.tilde       = as.vector(log((Z + 0.0000001)^2))
  Ytilde.alpha  = as.matrix(Y.tilde - alpha.st[as.vector(aux$s)])
  
  # sampling initial condition
  ############################################################
  V.h0.bar      = 1/((1 / priors$h0.v) + (1 / aux$sigma.v2))
  m.h0.bar      = V.h0.bar*((priors$h0.m / priors$h0.v) + (aux$H[1] / aux$sigma.v2))
  h0.draw       = rnorm(1, mean = m.h0.bar, sd = sqrt(V.h0.bar))
  aux$h0        = h0.draw
  
  # sampling sigma.v2
  ############################################################
  sigma.v2.s    = priors$sigmav.s + sum(c(aux$H[1] - aux$h0, diff(aux$H))^2)
  sigma.v2.draw = sigma.v2.s / rchisq(1, priors$sigmav.nu + T)
  aux$sigma.v2  = sigma.v2.draw
  
  # sampling auxiliary states
  ############################################################
  Pr.tmp        = simplify2array(lapply(1:10,function(x){
    dnorm(Y.tilde, mean = as.vector(aux$H + alpha.st[x]), sd = sqrt(sigma.st[x]), log = TRUE) + log(pi.st[x])
  }))
  Pr            = t(apply(Pr.tmp, 1, function(x){exp(x - max(x)) / sum(exp(x - max(x)))}))
  s.cum         = t(apply(Pr, 1, cumsum))
  r             = matrix(rep(runif(T), 10), ncol = 10)
  ss            = apply(s.cum < r, 1, sum) + 1
  aux$s         = as.matrix(ss)
  
  
  # sampling log-volatilities using functions for tridiagonal precision matrix
  ############################################################
  Sigma.s.inv   = diag(1 / sigma.st[as.vector(aux$s)])
  D.inv         = Sigma.s.inv + (1 / aux$sigma.v2) * priors$HH
  b             = as.matrix(Ytilde.alpha / sigma.st[as.vector(aux$s)] + (aux$h0/aux$sigma.v2)*diag(T)[,1])
  lead.diag     = diag(D.inv)
  sub.diag      = mgcv::sdiag(D.inv, -1)
  D.chol        = mgcv::trichol(ld = lead.diag, sd = sub.diag)
  D.L           = diag(D.chol$ld)
  mgcv::sdiag(D.L,-1) = D.chol$sd
  x             = as.matrix(rnorm(T))
  a             = forwardsolve(D.L, b)
  draw          = backsolve(t(D.L), a + x)
  aux$H         = as.matrix(draw)
  aux$sigma2    = as.matrix(exp(draw))
  
  return(aux)
}


# Setting specifications
N = ncol(Data[ , -1])
p = 12
K = 1+N*p
S = c(5000,25000)
h = 24
set.seed(1)

# Initializing X and Y matrices
y       = ts(Data[ , -1], start=c(2003,1), frequency=12)
Y       = ts(y[13:nrow(y),], start=c(2004,1), frequency=12)
T       = nrow(Y)
X       = matrix(1,nrow(Y),1)
for (i in 1:p){
  X     = cbind(X,y[13:nrow(y)-i,])
}


# Maximum Likelihood Estimator
A.hat       = solve(t(X)%*%X)%*%t(X)%*%Y
Sigma.hat   = t(Y-X%*%A.hat)%*%(Y-X%*%A.hat)/T

# Setting Minnesota Prior
kappa.1           = 0.02^2
kappa.2           = 100
A.prior           = matrix(0,nrow(A.hat),ncol(A.hat))
A.prior[2:(N+1),] = diag(N)
H                 = diag(T)
sdiag(H,-1)       = -1
HH                = 2*diag(T)
sdiag(HH,-1)      = -1
sdiag(HH,1)       = -1

priors = list(
  A.prior     = A.prior,
  V.prior     = diag(c(kappa.2,kappa.1*((1:p)^(-2))%x%rep(1,N))),
  S.prior     = diag(diag(Sigma.hat)),
  nu.prior    = N+1,
  
  # New priors based on lectures
  h0.v        = 1,
  h0.m        = 0,
  sigmav.s    = 1,
  sigmav.nu   = 1, 
  HH          = HH 
)

# BVAR function

BVAR.SV = function(Y,X,priors,S){

  aux <- list(
    Y = Y, 
    X = X,  
    H = matrix(1,T,1), 
    h0 = 0, 
    sigma.v2 = 1,
    s = matrix(1,T,1),
    A = matrix(0, K, N), 
    Sigma = diag(diag(matrix(1, N, N))),
    sigma2 = matrix(1, T, 1) 
  )
  
  A.posterior        = array(NA, dim = c(K,N,sum(S)))
  Sigma.posterior    = array(NA,dim=c(N,N,sum(S)))
  sigma2.posterior    = matrix(NA, nrow(Y), sum(S)) 
  
  for (s in 1:sum(S)){
    # normal-inverse Wishart posterior parameters
    V.bar.inv   = t(X)%*%diag(1/as.vector(aux$sigma2))%*%X + diag(1/diag(priors$V.prior))
    V.bar       = solve(V.bar.inv)
    A.bar       = V.bar%*%(t(X)%*%diag(1/as.vector(aux$sigma2))%*%Y + diag(1/diag(priors$V.prior))%*%priors$A.prior)
    nu.bar      = nrow(Y) + priors$nu.prior
    S.bar       = priors$S.prior + t(Y)%*%diag(1/as.vector(aux$sigma2))%*%Y + t(priors$A.prior)%*%diag(1/diag(priors$V.prior))%*%priors$A.prior - t(A.bar)%*%V.bar.inv%*%A.bar
    S.bar.inv   = solve(S.bar)
    
    #posterior draws
    Sigma.posterior   = rWishart(sum(S), df=nu.bar, Sigma=S.bar.inv)
    Sigma.posterior   = apply(Sigma.posterior,3,solve)
    Sigma.posterior   = array(Sigma.posterior,c(N,N,sum(S)))
    A.posterior       = array(rnorm(prod(c(dim(A.bar),sum(S)))),c(dim(A.bar),sum(S)))
    L                 = t(chol(V.bar))
    
    # Draw using stochastic volatility Gibbs common sampler
    A.posterior[,,s]= A.bar + L%*%A.posterior[,,s]%*%chol(Sigma.posterior[,,s])
    aux = SVcommon.Gibbs.iteration(aux, priors)
    sigma2.posterior[,s]  = aux$sigma2
    
  }
  
  posterior = list(
    Sigma.posterior   = Sigma.posterior,
    A.posterior       = A.posterior
  )
  return(posterior)
}

# Applying BVAR function
posterior.draws.SV = BVAR(Y=Y, X=X, priors=priors, S=S)
round(apply(posterior.draws.SV$Sigma.posterior, 1:2, mean),3)
round(apply(posterior.draws.SV$A.posterior, 1:2, mean),3)


```

# Bayesian VAR Forecasting

The aim is to forecast economic activity measured by industrial production two years into the future such that $h=24$. This is done for the baseline model and the stochastic volatility model for now:

## Bayesian VAR baseline model forecast

```{r}
#| echo: false
#| message: false
#| warning: false
#| result: hide

library(mvtnorm)
library(MASS)
library(HDInterval)

# simulate draws from the predictive density
h = 24 # 2 year ahead forecast
S = 30000
Y.h         = array(NA,c(h,N,S))

# sampling predictive density
for (s in 1:S){
    x.Ti        = Y[(nrow(Y)-h+1):nrow(Y),]
    x.Ti        = x.Ti[p:1,]
  for (i in 1:h){
    x.T         = c(1,as.vector(t(x.Ti)))
    Y.h[i,,s]   = rmvnorm(1, mean = x.T%*%posterior.draws$A.posterior[,,s], sigma=posterior.draws$Sigma.posterior[,,s])
    x.Ti        = rbind(Y.h[i,,s],x.Ti[1:(p-1),])
  }
}


gdp.point.f    = apply(Y.h[,1,],1,mean) 
gdp.interval.f = apply(Y.h[,1,],1,hdi,credMass=0.68)
gdp.range      = range(y[,1],gdp.interval.f)


```

```{r}
#| echo: false
#| message: false
#| warning: false
#| result: true

blue      = "#05386B"
green     = "#379683"
green.rgb = col2rgb(green)
shade     = rgb(green.rgb[1],green.rgb[2],green.rgb[3], alpha=120, maxColorValue=255)


par(mfrow=c(1,1), mar=rep(3,4),cex.axis=1.5)
plot(1:(length(y[,1])+h),c(y[,1],gdp.point.f), type="l", ylim=gdp.range, axes=FALSE, xlab="", ylab="", lwd=2, col=green)
axis(1,c(1,61,205, nrow(y),nrow(y)+h),c("2003-01","2008-01","2020-01","2023-12",""), col=blue)
axis(2,c(gdp.range[1],mean(gdp.range),gdp.range[2]),c("","GDP",""), col=blue)
abline(v=nrow(y), col="black")
text(x=253, y=4.8, srt=90, "2024-01")
abline(v=nrow(y)+12, col="black")
text(x=265, y=4.8, srt=90, "2025-01")
abline(v=nrow(y)+24, col="black")
text(x=277, y=4.8, srt=90, "2026-01")
polygon(c(length(y[,1]):(length(y[,1])+h),(length(y[,1]):(length(y[,1])+h))[25:1]),
        c(y[230,1],gdp.interval.f[1,],gdp.interval.f[2,24:1],y[230,1]),
        col=shade, border=blue)

```

## Bayesian VAR SV forecasting

```{r}
#| echo: false
#| message: false
#| warning: false
#| result: hide

h = 24 # 2 year ahead forecast
S = 30000
Y.h         = array(NA,c(h,N,S))

# sampling predictive density
for (s in 1:S){
  x.Ti        = Y[(nrow(Y)-h+1):nrow(Y),]
  x.Ti        = x.Ti[p:1,]
  for (i in 1:h){
    x.T         = c(1,as.vector(t(x.Ti)))
    Y.h[i,,s]   = rmvnorm(1, mean = x.T%*%posterior.draws.SV$A.posterior[,,s], sigma=posterior.draws.SV$Sigma.posterior[,,s])
    x.Ti        = rbind(Y.h[i,,s],x.Ti[1:(p-1),])
  }
}


gdp.point.f    = apply(Y.h[,1,],1,mean) 
gdp.interval.f = apply(Y.h[,1,],1,hdi,credMass=0.68)
gdp.range      = range(y[,1],gdp.interval.f)


```

```{r}
#| echo: false
#| message: false
#| warning: false
#| result: true

blue      = "#05386B"
green     = "#379683"
green.rgb = col2rgb(green)
shade     = rgb(green.rgb[1],green.rgb[2],green.rgb[3], alpha=120, maxColorValue=255)


par(mfrow=c(1,1), mar=rep(3,4),cex.axis=1.5)
plot(1:(length(y[,1])+h),c(y[,1],gdp.point.f), type="l", ylim=gdp.range, axes=FALSE, xlab="", ylab="", lwd=2, col=green)
axis(1,c(1,61,205, nrow(y),nrow(y)+h),c("2003-01","2008-01","2020-01","2023-12",""), col=blue)
axis(2,c(gdp.range[1],mean(gdp.range),gdp.range[2]),c("","GDP",""), col=blue)
abline(v=nrow(y), col="black")
text(x=253, y=4.8, srt=90, "2024-01")
abline(v=nrow(y)+12, col="black")
text(x=265, y=4.8, srt=90, "2025-01")
abline(v=nrow(y)+24, col="black")
text(x=277, y=4.8, srt=90, "2026-01")
polygon(c(length(y[,1]):(length(y[,1])+h),(length(y[,1]):(length(y[,1])+h))[25:1]),
        c(y[230,1],gdp.interval.f[1,],gdp.interval.f[2,24:1],y[230,1]),
        col=shade, border=blue)

```


# Showing models
To demonstrate that it works, I generate a bivariate Gaussian random walk process with 1 lag:

```{r}
#| echo: false
#| message: false
#| warning: false
#| result: true

### Showing baseline model
# Specifications
p = 1
N = 2
K = 1+N*p
S = 1000

# generate random walk processes
rw_data = data.frame(matrix(nrow=1000, ncol=2))
rw_data[,1] = cumsum(rnorm(1000,0,1))
rw_data[,2] = cumsum(rnorm(1000,0,1))
y       = matrix(cbind(rw_data[,1],rw_data[,2]),nrow=1000,ncol=N)

# Set X and Y matrices
Y       = ts(y[2:nrow(y),])
X       = matrix(1,nrow(Y),1)
X       = cbind(X,y[1:nrow(y)-p,])

# MLE
A.hat       = solve(t(X)%*%X)%*%t(X)%*%Y
Sigma.hat   = t(Y-X%*%A.hat)%*%(Y-X%*%A.hat)/nrow(Y)

# Minnesota prior
kappa.1           = 0.02^2
kappa.2           = 100
A.prior           = matrix(0,nrow(A.hat),ncol(A.hat))
A.prior[2:(N+1),] = diag(N)

priors = list(
  A.prior     = A.prior,
  V.prior     = diag(c(kappa.2,kappa.1*((1:p)^(-2))%x%rep(1,N))),
  S.prior     = diag(diag(Sigma.hat)),
  nu.prior    = N+1 
)

# Demonstration
posterior.draws.baseline = BVAR(Y=Y, X=X, priors=priors, S=S)
round(apply(posterior.draws.baseline$Sigma.posterior, 1:2, mean),3)
round(apply(posterior.draws.baseline$A.posterior, 1:2, mean),3)

plot(rw_data[,1], type='l', ylim=c(min(rw_data), max(rw_data)), col='red', ylab='', xlab='', main='Bivariate Random Walk')
lines(rw_data[,2], col='blue', ylab='', xlab='')

```

## References {.unnumbered}
